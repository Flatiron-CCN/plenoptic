{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representational Geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyrtools as pt\n",
    "import plenoptic as po\n",
    "from plenoptic.tools import to_numpy\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype  = torch.float32\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "einstein = po.load_images('../data/256/einstein.pgm')\n",
    "einstein = po.tools.conv.blur_downsample(einstein, step=(4,4))\n",
    "vid = po.tools.translation_sequence(einstein[0], n_steps=20)\n",
    "from torchvision.transforms.functional import center_crop\n",
    "vid = center_crop(vid, image_size // 2)\n",
    "vid = po.tools.rescale(vid, 0, 1)\n",
    "\n",
    "imgA = vid[0:1]\n",
    "imgB = vid[-1:]\n",
    "\n",
    "pt.image_stats(to_numpy(imgA))\n",
    "pt.image_stats(to_numpy(imgB))\n",
    "print(imgA.shape)\n",
    "print(vid.shape)\n",
    "\n",
    "# convention: full name for numpy arrays, short hands for torch tensors\n",
    "video = to_numpy(vid).squeeze()\n",
    "print(video.shape)\n",
    "# pt.animshow(video, zoom=4)\n",
    "pt.imshow(list(video.squeeze()), zoom=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral models\n",
    "Computing a geodesic to reveal excess invariance of the global Fourier magnitude representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft\n",
    "class Fourier(nn.Module):\n",
    "    def __init__(self, representation = 'amp'):\n",
    "        super().__init__()\n",
    "        self.representation = representation\n",
    "        \n",
    "    def spectrum(self, x):\n",
    "        return torch.fft.rfftn(x, dim=(2, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.representation == 'amp':\n",
    "            return torch.abs(self.spectrum(x))\n",
    "        elif self.representation == 'phase':\n",
    "            return torch.angle(self.spectrum(x))\n",
    "        elif self.representation == 'rectangular':\n",
    "            return self.spectrum(x)\n",
    "        elif self.representation == 'polar':\n",
    "            return torch.cat((torch.abs(self.spectrum(x)),\n",
    "                              torch.angle(self.spectrum(x))),\n",
    "                             dim=1)\n",
    "\n",
    "model = Fourier('amp')\n",
    "# model = Fourier('polar') # note: need pytorch>=1.8 to take gradients through torch.angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Normalize(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#     def forward(self, x):\n",
    "#         return torch.div(x, x.pow(2).sum(dim=(1,2,3), keepdim=True).pow(.5))\n",
    "# model = Normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = len(video)-1\n",
    "moog = po.synth.Geodesic(imgA, imgB, model, n_steps, init='bridge')\n",
    "moog.synthesize(max_iter=500, learning_rate=.01, lmbda=.1, mu=1, nu=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moog.plot_loss();\n",
    "moog.plot_deviation_from_line(vid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([po.to_numpy(e) for e in moog.step_energy], alpha=.2);\n",
    "plt.plot([e.mean() for e in moog.step_energy], 'r-', label='path energy')\n",
    "plt.axhline(torch.norm(moog._analyze(moog.xA) - moog._analyze(moog.xB)) ** 2 / moog.n_steps ** 2)\n",
    "plt.legend()\n",
    "plt.title('evolution of representation step energy')\n",
    "plt.ylabel('step energy')\n",
    "plt.xlabel('iteration')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    moog.step_jerkiness[0]\n",
    "    plt.plot([po.to_numpy(j) for j in moog.step_jerkiness]);\n",
    "    plt.plot([j.mean() for j in moog.step_jerkiness], 'r--', label='path energy');\n",
    "    plt.legend()\n",
    "    plt.title('evolution of representation step jerkiness')\n",
    "    plt.ylabel('step jerkiness')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "except:\n",
    "    plt.plot(moog.calculate_path_jerkiness())\n",
    "    plt.title('final representation step jerkiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(torch.stack([d[0] for d in moog.dev_from_line]));\n",
    "plt.plot(torch.stack([d[1] for d in moog.dev_from_line]));\n",
    "\n",
    "plt.title('evolution of distance from representation line')\n",
    "plt.ylabel('distance from representation line')\n",
    "plt.xlabel('iteration step')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelfade = to_numpy(moog.pixelfade.squeeze())\n",
    "geodesic = to_numpy(moog.geodesic.squeeze())\n",
    "fig = pt.imshow([video[5], pixelfade[5], geodesic[5]],\n",
    "          title=['video', 'pixelfade', 'geodesic'],\n",
    "          col_wrap=3, zoom=4);\n",
    "\n",
    "size = geodesic.shape[-1]\n",
    "h, m , l = (size//2 + size//4, size//2, size//2 - size//4)\n",
    "\n",
    "# for a in fig.get_axes()[0]:\n",
    "a = fig.get_axes()[0]\n",
    "for line in (h, m, l):\n",
    "    a.axhline(line, lw=2)\n",
    "\n",
    "pt.imshow([video[:,l], pixelfade[:,l], geodesic[:,l]],\n",
    "          title=None, col_wrap=3, zoom=4);\n",
    "pt.imshow([video[:,m], pixelfade[:,m], geodesic[:,m]],\n",
    "          title=None, col_wrap=3, zoom=4);\n",
    "pt.imshow([video[:,h], pixelfade[:,h], geodesic[:,h]],\n",
    "          title=None, col_wrap=3, zoom=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physiologically inspired models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = po.simul.OnOff(kernel_size=(31,31), pretrained=True)\n",
    "po.imshow(model(imgA), zoom=8);\n",
    "# po.imshow(model.conv.weight, zoom=28, vrange='auto0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "\n",
    "moog = po.synth.Geodesic(imgA, imgB, model, n_steps, init='bridge')\n",
    "\n",
    "print('shape trainable param', '# trainable param')\n",
    "sum(p.numel() for p in moog.parameters())\n",
    "[p.shape for p in moog.parameters() if p.requires_grad], sum(p.numel() for p in moog.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from adabelief_pytorch import AdaBelief\n",
    "    import adabelief_pytorch\n",
    "    print(adabelief_pytorch.__version__)\n",
    "    optimizer = AdaBelief([moog.x], lr=0.001, eps=1e-16, betas=(0.9,0.999),\n",
    "                          weight_decouple=True, rectify=False, print_change_log=False)\n",
    "except:\n",
    "    optimizer = 'Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moog.synthesize(optimizer=optimizer, nu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moog.plot_loss()\n",
    "moog.plot_deviation_from_line();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     moog.animate_distance_from_line(vid).save(\"../logs/distfromline_frontend_translation.mp4\")\n",
    "# except:\n",
    "#     print('generating the animation takes time, therefore we dont do it by default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moog.dev_from_line[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(po.to_numpy(torch.stack(moog., 0)[:, 1:-1]))\n",
    "plt.plot(torch.stack([d[0] for d in moog.dev_from_line]));\n",
    "# plt.plot(torch.stack([d[1][1:-1] for d in moog.dev_from_line]));\n",
    "\n",
    "plt.title('evolution of distance from representation line')\n",
    "plt.ylabel('distance from representation line')\n",
    "plt.xlabel('iteration step')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([po.to_numpy(e) for e in moog.step_energy]);\n",
    "plt.plot([e.mean() for e in moog.step_energy], 'r--', label='path energy')\n",
    "plt.axhline(torch.norm(moog._analyze(moog.xA) - moog._analyze(moog.xB)) ** 2 / moog.n_steps ** 2)\n",
    "plt.legend()\n",
    "plt.title('evolution of representation step energy')\n",
    "plt.ylabel('step energy')\n",
    "plt.xlabel('iteration')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    moog.step_jerkiness[0]\n",
    "    plt.plot([po.to_numpy(j) for j in moog.step_jerkiness]);\n",
    "    plt.plot([j.mean() for j in moog.step_jerkiness], 'r--', label='path energy');\n",
    "    plt.legend()\n",
    "    plt.title('evolution of representation step jerkiness')\n",
    "    plt.ylabel('step jerkiness')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "except:\n",
    "    plt.plot(moog.calculate_path_jerkiness())\n",
    "    plt.title('final representation step jerkiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodesic  = po.to_numpy(moog.geodesic).squeeze()\n",
    "pixelfade = po.to_numpy(moog.pixelfade).squeeze()\n",
    "assert geodesic.shape == pixelfade.shape\n",
    "geodesic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('geodesic')\n",
    "pt.imshow(list(geodesic), vrange='auto1', title=None, zoom=4);\n",
    "print('diff')\n",
    "pt.imshow(list(geodesic - pixelfade), vrange='auto1', title=None, zoom=4);\n",
    "print('pixelfade')\n",
    "pt.imshow(list(pixelfade), vrange='auto1', title=None, zoom=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that the range constraint is met\n",
    "plt.hist(video.flatten(), histtype='step', density=True, label='video')\n",
    "plt.hist(pixelfade.flatten(), histtype='step', density=True, label='pixelfade')\n",
    "plt.hist(geodesic.flatten(), histtype='step', density=True, label='geodesic');\n",
    "plt.yscale('log')\n",
    "plt.title('signal value histogram')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vgg16 translation / rotation / scaling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgA = po.load_images('../data/frontwindow_affine.jpeg', as_gray=False)\n",
    "imgB = po.load_images('../data/frontwindow.jpeg', as_gray=False)\n",
    "# imgA = torchvision.transforms.functional.center_crop(imgA, 224)\n",
    "# imgB = torchvision.transforms.functional.center_crop(imgB, 224)\n",
    "# torch.manual_seed()\n",
    "# imgA = torchvision.transforms.RandomCrop(224)(imgA)\n",
    "# imgB = torchvision.transforms.RandomCrop(224)(imgB)\n",
    "u = 300\n",
    "l = 90\n",
    "imgA = imgA[..., u:u+224, l:l+224]\n",
    "imgB = imgB[..., u:u+224, l:l+224]\n",
    "po.imshow([imgA, imgB], as_rgb=True);\n",
    "diff = imgA - imgB\n",
    "po.imshow(diff);\n",
    "pt.image_compare(po.to_numpy(imgA, True), po.to_numpy(imgB, True));\n",
    "# pt.image_stats(po.to_numpy(diff, True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgA = torch.tensor(imageA, dtype=dtype).unsqueeze(0).unsqueeze(0)\n",
    "# imgB = torch.tensor(imageB, dtype=dtype).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# # print(imgA.shape)\n",
    "# # from plenoptic.tools.straightness import make_straight_line\n",
    "# # n_steps = 11\n",
    "# # video = make_straight_line(imgA, imgB, n_steps)\n",
    "# # print(video.shape)\n",
    "# # pt.image_stats(po.to_numpy(video).squeeze())\n",
    "# # pt.animshow(po.to_numpy(video).squeeze(), zoom=2)\n",
    "\n",
    "# imgA = torch.stack([imgA, imgA, imgA], dim=1).squeeze(2)\n",
    "# imgB = torch.stack([imgB, imgB, imgB], dim=1).squeeze(2)\n",
    "# print(imgA.shape)\n",
    "# po.imshow([imgA, imgB], as_rgb=True, zoom=2);\n",
    "\n",
    "# # color_img = po.load_images('../data/color_wheel.jpg', as_gray=False)\n",
    "# # color_img = po.blur_downsample(color_img)\n",
    "# # color_img = po.blur_downsample(color_img)[..., 11:-11, 11:-11]\n",
    "# # color_img = po.blur_downsample(color_img)\n",
    "# # color_img = po.blur_downsample(color_img)\n",
    "# # imgA = po.rescale(color_img)\n",
    "# # imgB = torch.transpose(imgA.clone(), 2, 3)\n",
    "# # po.imshow([imgA, imgB], as_rgb=True, zoom=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "# Create a class that takes the nth layer output of a given model\n",
    "class NthLayer(torch.nn.Module):\n",
    "    \"\"\"Wrap any model to get the response of an intermediate layer\n",
    "    \n",
    "    Works for Resnet18 or VGG16.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, model, layer=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: PyTorch model\n",
    "        layer: int\n",
    "            Which model response layer to output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO\n",
    "        # is centrering appropriate??? \n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "        try:\n",
    "            # then this is VGG16\n",
    "            features = list(model.features)\n",
    "        except AttributeError:\n",
    "            # then it's resnet18\n",
    "            features = ([model.conv1, model.bn1, model.relu, model.maxpool] + [l for l in model.layer1] + \n",
    "                        [l for l in model.layer2] + [l for l in model.layer3] + [l for l in model.layer4] + \n",
    "                        [model.avgpool, model.fc])\n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "\n",
    "        if layer is None:\n",
    "            layer = len(self.features)\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.normalize(x)\n",
    "        for ii, mdl in enumerate(self.features):\n",
    "            x = mdl(x)\n",
    "            if ii == self.layer:\n",
    "                return x\n",
    "\n",
    "# different potential models of human visual perception of distortions\n",
    "# resnet18 = NthLayer(models.resnet18(pretrained=True), layer=3)\n",
    "\n",
    "# choosing what layer representation to study\n",
    "# for l in range(len(models.vgg16().features)):\n",
    "#     print(f'({l}) ', models.vgg16().features[l])   \n",
    "#     y = NthLayer(models.vgg16(pretrained=True), layer=l)(imgA) \n",
    "    # print(\"dim\", torch.numel(y), \"shape \", y.shape,)\n",
    "\n",
    "vgg_pool1 = NthLayer(models.vgg16(pretrained=True), layer=4)\n",
    "vgg_pool2 = NthLayer(models.vgg16(pretrained=True), layer=9)\n",
    "vgg_pool3 = NthLayer(models.vgg16(pretrained=True), layer=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of curiosity, if we are going to use a classifier\n",
    "# I wonder how sable the predicted label is along the geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predA = po.to_numpy(models.vgg16(pretrained=True)(imgA))[0]\n",
    "predB = po.to_numpy(models.vgg16(pretrained=True)(imgB))[0]\n",
    "\n",
    "plt.plot(predA);\n",
    "plt.plot(predB);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/aldebaran/Downloads/imagenet1000_clsidx_to_labels.txt\") as f:\n",
    "    idx2label = eval(f.read())\n",
    "\n",
    "for idx in np.argsort(predA)[-5:]:\n",
    "    print(idx2label[idx])\n",
    "for idx in np.argsort(predB)[-5:]:\n",
    "    print(idx2label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moog = po.synth.Geodesic(imgA, imgB, vgg_pool3)\n",
    "torch.numel(imgA), torch.numel(moog.model(imgA)), moog.model(imgA).shape, [p.shape for p in moog.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be run for longer on a GPU\n",
    "moog.synthesize(max_iter=50, learning_rate=.001, mu=1, nu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moog.plot_loss()\n",
    "moog.plot_deviation_from_line();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moog.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    moog.step_jerkiness[0]\n",
    "    plt.plot([po.to_numpy(j) for j in moog.step_jerkiness]);\n",
    "    plt.plot([j.mean() for j in moog.step_jerkiness], 'r--', label='path energy');\n",
    "    plt.legend()\n",
    "    plt.title('evolution of representation step jerkiness')\n",
    "    plt.ylabel('step jerkiness')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "except:\n",
    "    plt.plot(moog.calculate_path_jerkiness())\n",
    "    plt.title('final representation step jerkiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po.imshow(moog.geodesic, as_rgb=True, zoom=2, title=None, vrange='auto0');\n",
    "po.imshow(moog.pixelfade, as_rgb=True, zoom=2, title=None, vrange='auto0');\n",
    "# per channel difference\n",
    "po.imshow([(moog.geodesic - moog.pixelfade)[1:-1, 0:1]], zoom=2, title=None, vrange='auto1');\n",
    "po.imshow([(moog.geodesic - moog.pixelfade)[1:-1, 1:2]], zoom=2, title=None, vrange='auto1');\n",
    "po.imshow([(moog.geodesic - moog.pixelfade)[1:-1, 2:]], zoom=2, title=None, vrange='auto1');\n",
    "# exaggerated color difference\n",
    "po.imshow([po.rescale((moog.geodesic - moog.pixelfade)[1:-1])], as_rgb=True, zoom=2, title=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pick better anchor frames, here too small motion?\n",
    "# TODO investigate misbehaviour jerkiness while loss smoothly decreases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:atelier] *",
   "language": "python",
   "name": "conda-env-atelier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
